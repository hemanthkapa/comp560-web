# Literature

- Feng, G., Zhang, B., Gu, Y., Ye, H., He, D., &amp; Wang, L. (2023). Towards revealing the mystery behind chain of thought: a theoretical perspective. _Advances in Neural Information Processing Systems_, 36, 70757-70798. [pdf at neurips.cc](https://proceedings.neurips.cc/paper_files/paper/2023/file/dfc310e81992d2e4cedc09ac47eff13e-Paper-Conference.pdf). [Teams discussion channel](https://teams.microsoft.com/l/channel/19%3A6df87f59d4de410b8a2a75e94f6b0c7c%40thread.tacv2/Feng2023?groupId=327c5cf0-93db-45ce-be36-d648004b666f&tenantId=6232b055-76b9-4c13-9b88-b562ae7db6fb).

- J. MacCormick (2026). _Thinking AI: How Artificial Intelligence Emulates Human Understanding_. Princeton University Press. [chapter 10](https://dickinson0.sharepoint.com/:b:/r/sites/COMP560spring2026/Shared%20Documents/General/literature/maccormick-thinkingAI-ch10.pdf?csf=1&web=1&e=fuMXU7) (link is restricted to members the COMP560 MS Team), [PUP](https://press.princeton.edu/books/hardcover/9780691191737/thinking-ai), [Amazon](https://www.amazon.com/Thinking-AI-Artificial-Intelligence-Understanding-ebook/dp/B0FTKHZ676/).

- Lindsey, et al., "On the Biology of a Large Language Model", _Transformer Circuits_, 2025. [html at Anthropic](https://transformer-circuits.pub/2025/attribution-graphs/biology.html)

- Baeumel, Tanja, Josef van Genabith, and Simon Ostermann. "The lookahead limitation: Why multi-operand addition is hard for LLMs." arXiv preprint arXiv:2502.19981 (2025). [pdf at arXiv](https://arxiv.org/pdf/2502.19981).